{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5623M Assessment Coursework 1 - Image Classification [100 marks]\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Practice building, evaluating, and finetuning a convolutional neural network on an image dataset from development to testing. \n",
    "> 2. Gain a deeper understanding of feature maps and filters by visualizing some from a pre-trained network. \n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this provided template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process, especially for Question 1.3. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the ImageNet dataset [https://image-net.org/]. Our subset of Tiny ImageNet contains 30 different categories, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n",
    "\n",
    ">[Private class Kaggle competition and data](https://www.kaggle.com/t/9b703e0d71824a658e186d5f69960e27)\n",
    "\n",
    "To access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n",
    "\n",
    "Your student username (for example, ```sc15jb```):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> sc21kj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> KALYAN JOTHIMURUGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Feel free to add to this section as needed.\n",
    "\n",
    "You may need to download `cv2` using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "from csv import writer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU support\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "## QUESTION 1 [55 marks]\n",
    "\n",
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview:**\n",
    "*   **1.1.1** PyTorch ```Dataset``` and ```DataLoader``` classes\n",
    "*   **1.1.2** PyTorch ```Model``` class for simple CNN model\n",
    "*   **1.1.3** Overfitting on a single batch\n",
    "*   **1.2.1** Training on complete dataset\n",
    "*   **1.2.2** Fine-tuning model\n",
    "*   **1.2.3** Generating confusion matrices\n",
    "*   **1.3**   Testing on test set on Kaggle\n",
    "\n",
    "\n",
    "## 1.1 Single-batch training [14 marks]\n",
    "\n",
    "We will use a method of development called “single-batch training”, or \"overfitting a single batch\", in which we check that our model and the training code is working properly and can overfit a single training batch (i.e., we can drive the training loss to zero). Then we move on to training on the complete training set and adjust for any overfitting and fine-tune the model via regularisation.\n",
    "\n",
    "### 1.1.1 Dataset class [3 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your own root directory\n",
    "root=\"./data\"\n",
    "train_set_path = \"/train_set/train_set/\"\n",
    "test_set_path = \"/test_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own class LoadFromFolder\n",
    "# Below code is from the link provided in this template\n",
    "\n",
    "class LoadFromFolder(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "         \n",
    "        # Set the loading directory\n",
    "        self.main_dir =main_dir\n",
    "        self.transform = transform\n",
    "         \n",
    "        # List all images in folder and count them\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "    # Return the previously computed number of images\n",
    "     return len(self.total_imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        \n",
    "        # Use PIL for image loading\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        # Apply the transformations\n",
    "        tensor_image = self.transform(image)\n",
    "        \n",
    "        #return self.total_imgs[idx] ,torch.unsqueeze(tensor_image, 0)\n",
    "        #todo - above is modified\n",
    "        \n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 13500\n",
      "    Root location: ./data/train_set/train_set/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensorTransform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "single_batch_dataset = ImageFolder(root+train_set_path,transform = tensorTransform)\n",
    "\n",
    "print(single_batch_dataset)\n",
    "print(len(single_batch_dataset.classes))\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(single_batch_dataset))\n",
    "test_size = len(single_batch_dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(single_batch_dataset,\n",
    "                                                                  [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Define a CNN model [3 marks]\n",
    "\n",
    "Create a new model class using a combination of convolutional and fully connected layers, ReLU, and max-pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 3, 3])\n",
      "torch.Size([8])\n",
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([32, 16, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([128, 2048])\n",
      "torch.Size([128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "#conv2d parameters --> Conv2d(input_size, output_size, kernel_size, padding)\n",
    "\n",
    "cnnModel = nn.Sequential(\n",
    "    nn.Conv2d(3,8,3,1),    \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(8,16,3,1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(16,32,3,1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*8*8,128),   \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,30)\n",
    ")\n",
    "\n",
    "cnnModel = cnnModel.to(device)\n",
    "\n",
    "for param in cnnModel.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Single-batch training [8 marks]\n",
    "\n",
    "Write the foundational code which trains your network given **one single batch** of training data and computes the loss on the complete validation set for each epoch. Set ```batch_size = 64```. \n",
    "\n",
    "Display the graph of the training and validation loss over training epochs, showing as long as necessary to show you can drive the training loss to zero.\n",
    "\n",
    "> Please leave all graphs and code you would like to be marked clearly displayed without needing to run code cells or wait for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of loss and accuracy for given dataset loader and model. \n",
    "#This will be used for computing loss and accuracy on the test set after each training epoch.\n",
    "\n",
    "def stats(loader, cnnModel):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    n = 0    # count of minibatches\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            image_s, label_s = data\n",
    "            images, labels = image_s.to(device), label_s.to(device)\n",
    "            outputs = cnnModel(images)      \n",
    "            \n",
    "            # accumulate loss\n",
    "            running_loss += loss_fn(outputs, labels)\n",
    "            n += 1\n",
    "            \n",
    "            # accumulate data for accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)    # add in the number of labels in this minibatch\n",
    "            correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "            \n",
    "    return running_loss/n, correct/total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x1152 and 2048x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x1152 and 2048x128)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nepochs = 2\n",
    "results_path = root+\"/results/cnnclassifier1model.pt\"\n",
    "\n",
    "statsrec = np.zeros((4,nepochs))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnnModel.parameters(), lr=0.001, momentum=0.9)\n",
    "#inputs, labels = next(iter(train_loader))# single batch\n",
    "\n",
    "iteration = iter(train_loader)\n",
    "input_s, label_s = next(iteration)\n",
    "inputs, labels = input_s.to(device), label_s.to(device)\n",
    "\n",
    "for epoch in tqdm(range(nepochs)):  # loop over the dataset multiple times\n",
    "    correct = 0          # number of examples predicted correctly (for accuracy)\n",
    "    total = 0            # number of examples\n",
    "    running_loss = 0.0   # accumulated loss (for mean loss)\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward, backward, and update parameters\n",
    "    outputs = cnnModel(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "        # accumulate loss\n",
    "    running_loss = loss.item()\n",
    "        \n",
    "    # accumulate data for accuracy\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)    # add in the number of labels in this batch\n",
    "    correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "    \n",
    "    # collect together statistics for this epoch\n",
    "    ltrn = running_loss\n",
    "    atrn = correct/total \n",
    "    ltst, atst = stats(test_loader, cnnModel)\n",
    "    statsrec[:,epoch] = (ltrn, atrn, ltst, atst)\n",
    "    print(f\"epoch: {epoch} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\n",
    "\n",
    "# save network parameters, losses and accuracy\n",
    "torch.save({\"state_dict\": cnnModel.state_dict(), \"stats\": statsrec}, results_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Training and Test Loss and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training dataset epoch history\n",
    "data = torch.load(results_path)\n",
    "statsrec = data[\"stats\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(statsrec[0], 'r', label = 'training loss', )\n",
    "plt.plot(statsrec[2], 'g', label = 'test loss' )\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and test loss, and test accuracy')\n",
    "ax2=ax1.twinx()\n",
    "ax2.plot(statsrec[1], 'm', label = 'training accuracy')\n",
    "ax2.plot(statsrec[3], 'b', label = 'test accuracy')\n",
    "ax2.set_ylabel('accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "fig.savefig(\"roc.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.2 Training on complete dataset [23 marks]\n",
    "\n",
    "### 1.2.1 Train CNN and show loss graph [6 marks]\n",
    "\n",
    "Train your model on the complete training dataset, and use the validation set to determine when to stop training.\n",
    "\n",
    "Display the graph of training and validation loss over epochs to show how you determined the optimal number of training epochs.\n",
    "\n",
    "> As in previous sections, please leave the graph clearly displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nepochs = 300\n",
    "n_epochs_stop = 10\n",
    "epochs_no_improve = 0\n",
    "early_stop=False\n",
    "\n",
    "results_path_cnn200 = root+\"/results/cnnclassifier200model.pt\"\n",
    "\n",
    "statsrec = np.zeros((4,nepochs))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnnModel.parameters(), lr=0.001, momentum=0.9)\n",
    "min_val_loss=np.Inf\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "    correct = 0          # number of examples predicted correctly (for accuracy)\n",
    "    total = 0            # number of examples\n",
    "    running_loss = 0.0   # accumulated loss (for mean loss)\n",
    "    n = 0                # number of minibatches\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "         # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = cnnModel(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "        # accumulate data for accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)    \n",
    "        correct += (predicted == labels).sum().item()  \n",
    "    \n",
    "    ltrn = running_loss/n\n",
    "    atrn = correct/total \n",
    "    ltst, atst = stats(test_loader, cnnModel)\n",
    "    ### early stop if there is not change in validation loss over 10 instances of the epochs.\n",
    "    if ltst<min_val_loss:\n",
    "        epochs_no_improve = 0\n",
    "        min_val_loss = ltst\n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            early_stop = True\n",
    "            break\n",
    "    statsrec[:,epoch] = (ltrn, atrn, ltst, atst)\n",
    "    print(f\"epoch: {epoch} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\n",
    "\n",
    "# save network parameters, losses and accuracy\n",
    "torch.save({\"state_dict\": cnnModel.state_dict(), \"stats\": statsrec}, results_path_cnn200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2 = torch.load(results_path_cnn200)\n",
    "statsrec2 = data2[\"stats\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(statsrec2[0], 'r', label = 'training loss', )\n",
    "plt.plot(statsrec2[2], 'g', label = 'test loss' )\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and test loss, and test accuracy')\n",
    "ax2=ax1.twinx()\n",
    "ax2.plot(statsrec2[1], 'm', label = 'training accuracy')\n",
    "ax2.plot(statsrec2[3], 'b', label = 'test accuracy')\n",
    "ax2.set_ylabel('accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "fig.savefig(\"roc1.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.2 Finetuning [6 marks]\n",
    "\n",
    "Now finetune your architecture by implementing at least 2 methods of reducing overfitting and increasing the model's ability to generalise. You are encouraged to further adjust the model after you have done the minimum requirement, to increase your model performance. Please do not use any pre-trained weights from a model trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1:** Data augmentation of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2:** Adding dropout and/or batch normalisation to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you adjust the Model class, redefine it below and instantiate it as ```model_122a```, ```model_122b```, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trnsfrm_ft =  transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomGrayscale(p=0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ft= ImageFolder(root+train_set_path,transform=trnsfrm_ft)\n",
    "\n",
    "\n",
    "print(dataset_ft)\n",
    "print(len(dataset_ft.classes))\n",
    "train_size = int(0.8 * len(dataset_ft))\n",
    "validation_size = len(dataset_ft) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset_ft, [train_size, validation_size])## spliting train_set for both train_dataset and validation_set.\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(validation_dataset,batch_size=15, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding drop out\n",
    "model_122 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=3,padding=1),    # no padding, stride=1, dilation=1 by default\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Dropout(p=0.8),\n",
    "    nn.Conv2d(in_channels=8,out_channels=16,  kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=16,out_channels=32,  kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*8*8,128),     # with 32x32 input, the feature map size reduces to 8x8 with 16 channels.\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,30)\n",
    ")\n",
    "\n",
    "for param in model_122.parameters():\n",
    "    print(param.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats1(loader, model_122):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    n = 0    # counter for number of minibatches\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            outputs = model_122(images)      \n",
    "            \n",
    "            # accumulate loss\n",
    "            running_loss += loss_fn(outputs, labels)\n",
    "            n += 1\n",
    "            \n",
    "            # accumulate data for accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)    # add in the number of labels in this minibatch\n",
    "            correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "            \n",
    "    return running_loss/n, correct/total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nepochs = 200\n",
    "\n",
    "results_path_finetune= root+'/results/cnnfinetunedmodel.pt'\n",
    "n_epochs_stop = 15\n",
    "epochs_no_improve = 0\n",
    "early_stop=False\n",
    "\n",
    "statsrec = np.zeros((4,nepochs))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_122.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)## lr will change with patience 5\n",
    "min_val_loss=np.Inf\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "    correct = 0          # number of examples predicted correctly (for accuracy)\n",
    "    total = 0            # number of examples\n",
    "    running_loss = 0.0   # accumulated loss (for mean loss)\n",
    "    n = 0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "         # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = model_122(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "        # accumulate data for accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)    # add in the number of labels in this minibatch\n",
    "        correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "    # collect together statistics for this epoch\n",
    "    ltrn = running_loss/n\n",
    "    atrn = correct/total \n",
    "    ltst, atst = stats1(test_loader, model_122)\n",
    "    scheduler.step(ltst/len(test_loader))\n",
    "    ### early stop if there is not change in validation loss for 15 instance epochs.\n",
    "    if ltst<min_val_loss:\n",
    "        epochs_no_improve = 0\n",
    "        min_val_loss = ltst\n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            early_stop = True\n",
    "            break\n",
    "    statsrec[:,epoch] = (ltrn, atrn, ltst, atst)\n",
    "    print(f\"epoch: {epoch} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\n",
    "# save network parameters, losses and accuracy\n",
    "torch.save({\"state_dict\": model_122.state_dict(), \"stats\": statsrec}, results_path_finetune)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = torch.load(results_path_finetune)\n",
    "statsrec3 = data3[\"stats\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(statsrec3[0], 'r', label = 'training loss', )\n",
    "plt.plot(statsrec3[2], 'g', label = 'test loss' )\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and test loss, and test accuracy')\n",
    "ax2=ax1.twinx()\n",
    "ax2.plot(statsrec3[1], 'm', label = 'training accuracy')\n",
    "ax2.plot(statsrec3[3], 'b', label = 'test accuracy')\n",
    "ax2.set_ylabel('accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "fig.savefig(\"roc1.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.3 Training comparison [4 marks]\n",
    "\n",
    "Display, side-by-side or on one single graph, the training and validation loss graphs for the single-batch training (section 1.1.3), on the full training set (1.2.1) and your final fine-tuned model (1.2.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "plt.plot(statsrec[0], 'r', label = 'single-batch training', marker='o', alpha=0.4)\n",
    "plt.plot(statsrec[2], 'g', label = 'batch validation', marker='o', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.plot(statsrec2[0], 'r', label = 'full training', marker='*', alpha=0.4)\n",
    "plt.plot(statsrec2[2], 'g', label = 'full validation' , marker='*', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.plot(statsrec3[0], 'r', label = 'fine-tuned training', marker='d', alpha=0.4)\n",
    "plt.plot(statsrec3[2], 'g', label = 'fine-tuned validation', marker='d', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.legend(loc='center right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what can be seen in the graphs.\n",
    "\n",
    "--> Double click here to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.4 Confusion matrices [7 marks]\n",
    "\n",
    "Use your architecture with best accuracy to generate two confusion matrices, one for the training set and one for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_names = (pd.read_csv(root+'/mapping.txt',header = None, sep =\"\\t\")).drop(columns=[2])\n",
    "categories_names = categories_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_confusion_matrix(model, loader, categories_name):\n",
    "    all_preds = torch.tensor([])\n",
    "    all_labels = torch.tensor([])\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            image_s, label_s = images.to(device), labels.to(device)\n",
    "\n",
    "            preds = model(image_s)\n",
    "\n",
    "            all_preds = torch.cat((all_preds.to(device), preds),dim=0)\n",
    "            all_labels = torch.cat((all_labels.to(device), label_s),dim=0)\n",
    "            fig, ax = plt.subplots(figsize=(25, 25))\n",
    "            cm = confusion_matrix(all_labels.tolist(), all_preds.argmax(dim=1).tolist())\n",
    "\n",
    "    conf_matrix=pd.DataFrame(data=cm, columns=classes, index=classes)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(model_122, train_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Classes\n",
    "classes  = list()\n",
    "labels = open(root+\"/mapping.txt\")\n",
    "for map in labels:\n",
    "  key, value =map.split()\n",
    "  classes.append(value)\n",
    "classes = tuple(classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions can be drawn from the confusion matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Double click to respond here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.3 Testing on test data [18 marks]\n",
    "\n",
    "### 1.3.1 Dataset and generating predictions [6 marks]\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        input_s, label_s = data\n",
    "        inputs, labels = input_s.to(device), label_s.to(device)\n",
    "\n",
    "        outputs = model_122(inputs)\n",
    "        results.extend(outputs.argmax(dim=1).type(torch.int32).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3.2 CSV file and test set accuracy [12 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image on test set and 1 row for the headers.\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [10 marks]. The class leaderboard will not affect marking (brownie points!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/AI/sc21vp.csv\", 'a', newline='') as f_object:  \n",
    "    # Pass the CSV  file object to the writer() function\n",
    "    writer_object = writer(f_object)\n",
    "    # Result - a writer object\n",
    "    # Pass the data in the list as an argument into the writerow() function\n",
    "    for list_data in predicted_list:\n",
    "     writer_object.writerow(list_data)  \n",
    "    # Close the file object\n",
    "    f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = validation_dataset.imgs\n",
    "names = []\n",
    "for path in folder_names:\n",
    "    names.append(path[0].split('/')[-1])\n",
    "    names[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sR8Gc04CJ2"
   },
   "source": [
    "\n",
    "\n",
    "## QUESTION 2 [40 marks]\n",
    "\n",
    "\n",
    "\n",
    "In this question, you will visualize the filters and feature maps of a fully-trained CNN (AlexNet) on the full ImageNet 2012 dataset.\n",
    "\n",
    "> Please do not alter the name of the function or the number and type of its arguments and return values, otherwise the automatic grading function will not work correctly. You are welcome to import other modules (though the simplest solution only requires the ones below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-HIUgQ-HK8Y"
   },
   "source": [
    "### **Overview:**\n",
    "*   **2.1.1** Extract filters from model: ``fetch_filters(layer_idx, model)``\n",
    "*   **2.2.1** Load test image\n",
    "*   **2.2.2** Extract feature maps for given test image: ``fetch_feature_maps(image, model)``\n",
    "*   **2.2.3** Display feature maps\n",
    "*   **2.3.1** Generate Grad-CAM heatmaps: ``generate_heatmap(output, class_id, model, image)``\n",
    "*   **2.3.2** Display heatmaps: add code to cell\n",
    "*   **2.3.3** Generate heatmaps for failure analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gYdjXng4CJ5"
   },
   "source": [
    "### Loading a pre-trained model\n",
    "\n",
    "Run the cell below to load an AlexNet model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik9dzD4S4CJ6"
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXzlLbstCE7f"
   },
   "source": [
    "\n",
    "## 2.1 Extract and visualize the filters [6 marks]\n",
    "\n",
    "In this section you will extract and visualize the filters from the pre-trained AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vOrqr2J4CJ7"
   },
   "source": [
    "### 2.1.1 Extract filters [4 marks]\n",
    "\n",
    "Complete the following function ```fetch_filters``` to return all the filters from the convolutional layers at the given index in ```model.features``` (see printed model above for reference). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> We will not test the behaviour of your function using invalid indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdbDXckn4CJ8"
   },
   "outputs": [],
   "source": [
    "def fetch_filters(layer_idx, model):\n",
    "    \"\"\" \n",
    "        Args:\n",
    "            layer_idx (int): the index of model.features specifying which conv layer\n",
    "            model (AlexNet): PyTorch AlexNet object\n",
    "        Return:\n",
    "            filters (Tensor):      \n",
    "    \"\"\"\n",
    "    return model.features[layer_idx].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNxPI5y-4CJ8"
   },
   "outputs": [],
   "source": [
    "# all the indices of the conv layers\n",
    "conv_layer_idx = [0, 3, 6, 8, 10]\n",
    "\n",
    "filters = []\n",
    "\n",
    "for layer_idx in conv_layer_idx:\n",
    "    filters.append(fetch_filters(layer_idx, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWSpX94c4CKD"
   },
   "source": [
    "For your testing purposes, the following code blocks test the dimensions of the function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXeQtKkK4CKE"
   },
   "outputs": [],
   "source": [
    "filters[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZnC8Eth4CKF"
   },
   "outputs": [],
   "source": [
    "assert list(filters[0].shape) == [64, 3, 11, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECx7Ktsg4CKG"
   },
   "source": [
    "\n",
    "\n",
    "### 2.1.2 Display filters [2 marks]\n",
    "\n",
    "The following code will visualize some of the filters from each layer. Play around with viewing filters at different depths into the network. Note that ```filters[0]``` could be viewed in colour if you prefer, whereas the subsequent layers must be viewed one channel at a time in grayscale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K6N3ThU4CKG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# limit how many filters to show\n",
    "to_show = 16\n",
    "\n",
    "# compute the dimensions of the plot\n",
    "plt_dim = int(math.sqrt(to_show))\n",
    "\n",
    "# plot the first channel of each filter in a grid\n",
    "for i, filt in enumerate(filters[0].numpy()[:to_show]):\n",
    "    plt.subplot(plt_dim, plt_dim, i+1)\n",
    "    plt.imshow(filt[0], cmap=\"Greens\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCJF9IIF4CKI"
   },
   "source": [
    "\n",
    "\n",
    "## 2.2 Extract and visualize feature maps [10 marks]\n",
    "\n",
    "In this section, you will pass a test image through the AlexNet and extract and visualize the resulting convolutional layer feature maps.\n",
    "\n",
    "Complete the following code cell to load the test image ```man_bike.JPEG```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVfEgbC4I_dE"
   },
   "source": [
    "### 2.2.1 Load test image [1 mark]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xypfUN7y4CKI"
   },
   "outputs": [],
   "source": [
    "#load image\n",
    "im = Image.open(root+'/man_bike.JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF2t9uOk4CKJ"
   },
   "source": [
    "Run the code cell below to apply the image transformation expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt0tJQsM4CKO"
   },
   "outputs": [],
   "source": [
    "# ImageNet normalisation values, to apply to the image transform\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std),\n",
    "    ])\n",
    "\n",
    "im = data_transform(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQKNo384CKP"
   },
   "source": [
    "\n",
    "### 2.2.2 Extract feature maps [5 marks]\n",
    "\n",
    "Complete the function below to pass the test image through a single forward pass of the network. We are interested in the outputs of the max pool layers (outputs of conv layers at model.features indices 0, 3, and 10) for best visualization. Note that the input should pass through *every layer* of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmqQ_mJ54CKP"
   },
   "outputs": [],
   "source": [
    "def fetch_feature_maps(image, model):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image (Tensor): a single input image with transform applied\n",
    "        model (AlexNet): PyTorch AlexNet object\n",
    "        \n",
    "    Return:\n",
    "        feature_maps (Tensor): all the feature maps from conv layers \n",
    "                    at indices 0, 3, and 10 (outputs of the MaxPool layers)\n",
    "    \"\"\"\n",
    "\n",
    "    model_weights=[]\n",
    "    conv_layers=[]\n",
    "    all_layers=[]\n",
    "    maxpool=[]\n",
    "    counter = 0\n",
    "    model_children = list(model.children())\n",
    "\n",
    "    model_children = list(model.children())\n",
    "\n",
    "\n",
    "   \n",
    "    for j in range(len(model_children[0])):\n",
    "        child= model_children[0][j]\n",
    "        all_layers.append(child)\n",
    "        if type(child) == nn.MaxPool2d:\n",
    "            counter += 1\n",
    "            conv_layers.append(child)\n",
    "\n",
    "\n",
    "\n",
    "    results = [all_layers[0](image)] # -> (1, 1, 64, 63, 63) first dim to save each result\n",
    "  \n",
    "    for i in range(1, len(all_layers)):\n",
    "        # use last result to calculate next function\n",
    "        x=all_layers[i](results[-1])\n",
    "        # each function's result will be save in the list of result\n",
    "        results.append(x)\n",
    "        # after MaxPool, Height(or Width) will be half\n",
    "        if len(results[i][0][0]) == len(results[i-1][0][0])// 2:\n",
    "            maxpool.append(x)\n",
    "       \n",
    "    # transfer to Tensor\n",
    "    maxpool=[torch.tensor(x,requires_grad=False) for x in maxpool ]\n",
    "    \n",
    "    return maxpool, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORkRxCVo4CKQ"
   },
   "outputs": [],
   "source": [
    "feature_maps = fetch_feature_maps(im.unsqueeze(0), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf3SZoFu4CKQ"
   },
   "source": [
    "For your testing purposes, the following code block tests the dimensions of part of the function output. Note that the first dimension is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow7jGdQ94CKR"
   },
   "outputs": [],
   "source": [
    "assert len(feature_maps) == 3\n",
    "assert list(feature_maps[0].shape) == [1, 64, 31, 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDmwrp-w4CKR"
   },
   "source": [
    "\n",
    "\n",
    "### 2.2.3 Display feature maps [4 marks]\n",
    "\n",
    "Using the code for displaying filters as reference, write code in the block below to display the outputs of the first **16 feature maps from each of the 3 max-pool layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2O8TZG74CKS"
   },
   "outputs": [],
   "source": [
    "feature_map_size = 16\n",
    "\n",
    "to_show = feature_map_size\n",
    "\n",
    "# compute the dimensions of the plot\n",
    "plt_dim = int(math.sqrt(to_show))\n",
    "\n",
    "# plot the first channel of each filter in a grid\n",
    "for i, filt in enumerate(feature_maps[0][0].numpy()[:to_show]):\n",
    "    plt.subplot(plt_dim, plt_dim, i+1)\n",
    "    plt.imshow(filt, cmap=\"Blues\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZNGf5WQ4CKG"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2.3 Understanding of filters and feature maps [7 marks]\n",
    "\n",
    "Respond in detail to the questions below. (Note that all text boxes can be formatted using Markdown if desired).\n",
    "\n",
    "### 2.3.1 [3 marks]\n",
    "Describe what the three filters at indices 0, 4, and 6 from the first convolutional layer are detecting (reference the corresponding feature maps to support your discussion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Filter 0 -> It is classifying and grouping man and block as one element. Able to see the silhoute of the man as part of fore ground. It also detects foreground elements.\n",
    "\n",
    "--> Filter 4 -> It detects much clearer picture of man's head and his torso.\n",
    "\n",
    "--> Filter 6 -> Able to identify the wheels of the cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 [2 marks]\n",
    "Discuss how the filters change with depth into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> At first, the filters detect multiple colors and edges and is able to identify different shapes.\n",
    "\n",
    "--> At final layers, we can see more clearer image with more complex patterns that is used to identify the element in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 [2 marks]\n",
    "Discuss how the feature maps change with depth into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> As depth increases, the image is broken down into simpler blocks.\n",
    "--> At first, we will be able to see and differentiate between foreground and background scenes, but as the depth increases, it will be reduced and will be able to only see shapes that appears in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXeO_agI4CKS"
   },
   "source": [
    "\n",
    "## 2.4 Gradient-weighted Class Activation Mapping (Grad-CAM) [17 marks]\n",
    "\n",
    "In this section, we will explore using Gradient-weighted Class Activation Mapping (Grad-CAM) to generate coarse localization maps highlighting the important regions in the test images guiding the model's prediction. We will continue using the pre-trained AlexNet.\n",
    "\n",
    "#### Preparation\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "#### The AlexNet class\n",
    "\n",
    ">To implement Grad-CAM, we need to edit the AlexNet ```module``` class itself, so instead of loading the AlexNet model from ```torch.hub``` as we did above, we will use the official PyTorch AlexNet class code ([taken from here](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html)). In addition to the class definition, there is also a function below called ```alexnet()``` which allows you to specify whether you want the pretrained version or not, and if so, loads the weights. \n",
    "\n",
    "#### The hook\n",
    "\n",
    ">[Hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) in PyTorch are functions which can be registered, or attached, to a ```Module``` or ```Tensor```. Hooks can be *forward* hooks or *backward* hooks; forward hooks are called with ```forward()``` and backward hooks with ```backward()```. In the model below, we register a forward hook that saves the **gradients of the activations** to the Tensor output of ```model.features```. The gradients are saved to a class variable so we can easily access them.\n",
    "\n",
    "Carefully read the code block below. You do not need to add anything to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74G4wPeG4CKS"
   },
   "outputs": [],
   "source": [
    "# defining where to load the pre-trained weights from\n",
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-7be5be79.pth',\n",
    "}\n",
    "\n",
    "# the class definition\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # a placeholder for storing the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    # the hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        # stores the gradients of the hook's tensor to our placeholder variable\n",
    "        self.gradients = grad\n",
    "\n",
    "    # a method for extracting the activations of the last conv layer only (when we're \n",
    "    # not interested in a full forward pass)\n",
    "    def get_activations(self, x):\n",
    "        return self.features(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # we register the hook here to save the gradients of the last convolutional\n",
    "        # layer outputs\n",
    "        hook = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(pretrained=False, progress=True, **kwargs) -> AlexNet:\n",
    "    \"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FZEiLFv4CKT"
   },
   "outputs": [],
   "source": [
    "model = alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass our test image through our new model with the hook\n",
    "output = model(im.unsqueeze(0))\n",
    "\n",
    "# save the predicted class\n",
    "_, pred_cls = output.max(dim=1, keepdim=True)\n",
    "\n",
    "print(output.shape)\n",
    "print(pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzCYhGyI4CKT"
   },
   "source": [
    "Examine and understand the values stored in ```output``` and ```pred_cls```. What does AlexNet classify the test image as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU5QjsuP4CKU"
   },
   "outputs": [],
   "source": [
    "output.shape # torch.Size([1, 1000])\n",
    "max(output.detach().numpy().tolist()[0])\n",
    "output.detach().numpy().tolist()[0].index(13.724449157714844)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1000 classes that any image can be classified into.\n",
    "pred_cls has the value of 671, which maps to mountain bike, all-terrai bike, off-roader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr4svXcg4CKU"
   },
   "source": [
    "### 2.4.1 Generate Grad-CAM heatmaps [8 marks]\n",
    "\n",
    "With the hooks in place, now implement the code to generate Grad-CAM heatmaps, by following the guiding comments in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdlr69tw4CKU"
   },
   "outputs": [],
   "source": [
    "def display_heatmap(heatmap,image):\n",
    "    (w, h) = (image.shape[0], image.shape[1])\n",
    "    heatmap = cv2.resize(heatmap.numpy(), (w, h))\n",
    "\n",
    "    numer = heatmap - np.min(heatmap)\n",
    "    denom = (heatmap.max() - heatmap.min()) + 1e-8\n",
    "    heatmap_normalized = numer / denom\n",
    "    heatmap_normalized = (heatmap_normalized * 255).astype(\"uint8\")\n",
    "    \n",
    "    heatmap_normalized = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap_normalized=heatmap_normalized * 0.4\n",
    "    \n",
    "    #heatmap_normalized=heatmap_normalized.reshape((256,256,1))*np.ones([256,256,3])\n",
    "    weighted_image= cv2.addWeighted(heatmap_normalized, 0.7, image, 0.3, 0)\n",
    "    \n",
    "    numer = weighted_image - np.min(weighted_image)\n",
    "    denom = (weighted_image.max() - weighted_image.min()) + 1e-8\n",
    "    final_image = numer / denom\n",
    "    final_image = (final_image * 255).astype(\"uint8\")\n",
    "\n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PEmi-aM4CKU"
   },
   "outputs": [],
   "source": [
    "heatmap = generate_heatmap(output, pred_cls, model, im.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaiH3MIO4CKV"
   },
   "source": [
    "Check the dimensions of ```heatmap```. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qigb0A9F4CKV"
   },
   "outputs": [],
   "source": [
    "print(heatmap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvyCYUpw4CKW"
   },
   "source": [
    "### 2.4.2 Display heatmaps [4 marks]\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap super-imposed onto the original image. To get results as shown in the paper, we recommend the following steps:\n",
    "\n",
    "1. Resize the heatmap to match the size of the image.\n",
    "2. Rescale the image to a 0-255 integer range.\n",
    "3. Apply a colormap to the heatmap using ```cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)```.\n",
    "4. Multiply all values of heatmap by 0.4 to reduce colour saturation.\n",
    "5. Superimpose the heatmap onto the original image (Note: please perform cv2's addition - addition of two cv2 images, not numpy addition. See [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.html#:~:text=addWeighted()%20etc.-,Image%20Addition,OpenCV%20addition%20and%20Numpy%20addition.) for explanation.)\n",
    "6. Normalize the image between 0-255 again.\n",
    "7. Display the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwTIJoh84CKX"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "\n",
    "def display_heatmap(image, heatmap):\n",
    "    heatmap = cv2.resize(heatmap.numpy(), (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    heatmap = cv2.normalize(heatmap,  None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    \n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap = heatmap * 0.4\n",
    "    heatmap = np.uint8(heatmap)\n",
    "    \n",
    "    superimposed = cv2.add(image, heatmap)\n",
    "    \n",
    "    superimposed = cv2.normalize(superimposed,  None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    plt.imshow(superimposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(root+\"/man_bike.JPEG\")\n",
    "image = cv2.resize(image, (256, 256))\n",
    "display_heatmap(image, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf9aa-2V4CKX"
   },
   "source": [
    "Show the heatmap for class ```'seashore, coast, seacoast, sea-coast'``` (```class_id = 978```), super-imposed onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_heatmap = generate_heatmap(output, torch.tensor([[978]]), model, im.unsqueeze(0))\n",
    "retain_graph=True\n",
    "display_heatmap(image, sea_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Failure analysis using Grad-CAM [5 marks]\n",
    "\n",
    "Find an image (online, or from ImageNet or another dataset) which AlexNet classifies *incorrectly*. Display the image below, and show the model's predicted class. Then, generate the Grad-CAM heatmap and display it super-imposed onto the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamooth=Image.open(root+'mammoth.jpg')\n",
    "\n",
    "# ImageNet normalisation values, to apply to the image transform\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std),\n",
    "    ])\n",
    "\n",
    "mamooth = data_transform(mamooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode001 = alexnet(pretrained=True)\n",
    "\n",
    "# pass our test image through our new model with the hook\n",
    "output = mode001(mamooth.unsqueeze(0))\n",
    "\n",
    "# save the predicted class\n",
    "_, pred_cls = output.max(dim=1, keepdim=True)\n",
    "\n",
    "pred_cls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = torch.nn.functional.softmax(output, dim=1)[0] * 100  \n",
    "print(classes.iloc[pred_cls.numpy()[0][0]], percentage[pred_cls[0][0]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = generate_heatmap(output, pred_cls, mode001, mammoth.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamooth=cv2.imread(root+'mammoth.jpg')\n",
    "\n",
    "mamooth=cv2.resize(mamooth,(256,256))\n",
    "mamooth = np.asarray(mamooth, np.float64)\n",
    "show_result = display_heatmap(heatmap, mamooth)\n",
    "plt.matshow(show_result[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe what explanation the Grad-CAM heatmap provides about why the model has failed to correctly classify your test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Double click to respond here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Overall quality [2 marks]\n",
    "\n",
    "Marks awarded for overall degree of code readibility and omission of unnecessary messy outupts (for example, please avoid printed losses for every batch of a long training process, large numpy arrays, etc.) throughout the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
