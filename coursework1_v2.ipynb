{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5623M Assessment Coursework 1 - Image Classification [100 marks]\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Practice building, evaluating, and finetuning a convolutional neural network on an image dataset from development to testing. \n",
    "> 2. Gain a deeper understanding of feature maps and filters by visualizing some from a pre-trained network. \n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this provided template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process, especially for Question 1.3. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the ImageNet dataset [https://image-net.org/]. Our subset of Tiny ImageNet contains 30 different categories, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n",
    "\n",
    ">[Private class Kaggle competition and data](https://www.kaggle.com/t/9b703e0d71824a658e186d5f69960e27)\n",
    "\n",
    "To access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n",
    "\n",
    "Your student username (for example, ```sc15jb```):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> sc21kj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Kalyan Jothimurugan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Feel free to add to this section as needed.\n",
    "\n",
    "You may need to download `cv2` using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with your own file path\n",
    "root = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "## QUESTION 1 [55 marks]\n",
    "\n",
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview:**\n",
    "*   **1.1.1** PyTorch ```Dataset``` and ```DataLoader``` classes\n",
    "*   **1.1.2** PyTorch ```Model``` class for simple CNN model\n",
    "*   **1.1.3** Overfitting on a single batch\n",
    "*   **1.2.1** Training on complete dataset\n",
    "*   **1.2.2** Fine-tuning model\n",
    "*   **1.2.3** Generating confusion matrices\n",
    "*   **1.3**   Testing on test set on Kaggle\n",
    "\n",
    "\n",
    "## 1.1 Single-batch training [14 marks]\n",
    "\n",
    "We will use a method of development called “single-batch training”, or \"overfitting a single batch\", in which we check that our model and the training code is working properly and can overfit a single training batch (i.e., we can drive the training loss to zero). Then we move on to training on the complete training set and adjust for any overfitting and fine-tune the model via regularisation.\n",
    "\n",
    "### 1.1.1 Dataset class [3 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Loading datasets\n",
    "train_dataset = datasets.ImageFolder(root+'/train_set/train_set/', \n",
    "                                     transform = transform)\n",
    "test_dataset = datasets.ImageFolder(root+'/test_set/',\n",
    "                                    transform = transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('baboon', 'banana', 'bee', 'bison', 'butterfly', 'candle', 'cardigan', 'chihuahua', 'elephant', 'espresso', 'fly', 'goldfish', 'goose', 'grasshopper', 'hourglass', 'icecream', 'ipod', 'jellyfish', 'koala', 'ladybug', 'lion', 'mushroom', 'penguin', 'pig', 'pizza', 'pretzel', 'redpanda', 'refrigerator', 'sombrero', 'umbrella')\n"
     ]
    }
   ],
   "source": [
    "# Loading the Classes\n",
    "classes  = list()\n",
    "labels = open(root+\"/mapping.txt\")\n",
    "for map in labels:\n",
    "    key, value =map.split()\n",
    "    classes.append(value)\n",
    "classes = tuple(classes) \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check -1\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "#imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Define a CNN model [3 marks]\n",
    "\n",
    "Create a new model class using a combination of convolutional and fully connected layers, ReLU, and max-pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code to calculate Loss and Accuracy\n",
    "def stats(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    n = 0    # counter for number of minibatches\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            input_s, label_s = data\n",
    "            inputs, labels = input_s.to(device), label_s.to(device)\n",
    "\n",
    "            outputs = net(inputs)      \n",
    "            \n",
    "            # accumulate loss\n",
    "            running_loss += loss_fn(outputs, labels)\n",
    "            n += 1\n",
    "            \n",
    "            # accumulate data for accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)    # add in the number of labels in this minibatch\n",
    "            correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "\n",
    "    return running_loss/n, correct/total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu5): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=6400, out_features=128, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1)\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3=nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu4=nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu5=nn.ReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64*10*10, 128)\n",
    "        self.relu6=nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 30)\n",
    "        # self.relu4=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))\n",
    "        x = self.relu5(self.conv5(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc2(self.relu6(self.fc1(x)))\n",
    "        return x\n",
    "\n",
    "net01 = Net()\n",
    "net01.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet01(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "        \n",
    "#         #layer 1\n",
    "#         self.conv1 = nn.Conv2d(input_layer = 3, out_layer = 16, kernel_size = 5)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#        # self.pool1 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "#         #layer 2\n",
    "#         self.conv2 = nn.Conv2d(input_layer = 16, out_layer = 16, kernel_size = 5)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "#         #layer 3\n",
    "#         self.conv3 = nn.Conv2d(input_layer = 16, out_layer = 32, kernel_size = 3)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         #self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "#         #layer 4\n",
    "#         self.conv4 = nn.Conv2d(input_layer = 32, out_layer = 64, kernel_size = 3)\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.pool4 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        \n",
    "#         #fully connected layer\n",
    "#         #self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(64*10*10, 128)\n",
    "#         self.relu5 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(128, 30)\n",
    "# #         self.fc2 = nn.Linear(128,84)\n",
    "# #         self.relu6 = nn.ReLU()\n",
    "# #         self.fc3 = nn.Linear(84,30)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.conv1(x))\n",
    "#         x = self.pool2(F.relu2(self.conv2(x)))\n",
    "#         x = self.pool3(F.relu3(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu4(self.conv4(x)))\n",
    "\n",
    "#         #x = self.flatten(x)\n",
    "#         x = F.relu5(self.fc1(x))\n",
    "#         x = F.relu6(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnModel = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet00(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, 5, stride = 1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         #self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(32, 32, 5, stride = 1)\n",
    "#         self.relu2=nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(64,64,3)\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(64, 64, 3)\n",
    "#         self.relu5 = nn.ReLU()\n",
    "        \n",
    "#         self.flatten = nn.Flatten()\n",
    "        \n",
    "#         self.fc1 = nn.Linear(64 * 10 * 10, 128)\n",
    "#         self.relu6 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(128, 30)\n",
    "#         #self.fc3 = nn.Linear(84, 30)\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = self.relu1(self.conv1(x))\n",
    "# #         x = self.pool2(self.relu2(self.conv2(x)))\n",
    "# #         x = self.relu3(self.conv3(x))\n",
    "# #         x = self.pool4(self.relu4(self.conv4(x)))\n",
    "# #         x = self.flatten(x)\n",
    "# #         x = self.fc2(self.relu6(self.fc1(x)))\n",
    "        \n",
    "#         x = self.pool(F.relu(self.conv1(x))) \n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "#         x = x.view(-1, 64 * 10 * 10)            \n",
    "#         x = F.relu(self.fc1(x))               \n",
    "#         x = F.relu(self.fc2(x))               \n",
    "#         x = self.fc3(x)                       \n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnmodel02 = ConvNet00()\n",
    "# cnnmodel02.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet02(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         print('four')\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1)\n",
    "#         self.relu1=nn.ReLU()\n",
    "\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1)\n",
    "#         self.relu2=nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "#         self.relu3=nn.ReLU()\n",
    "\n",
    "#         self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "#         self.relu4=nn.ReLU()\n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "#         self.relu5=nn.ReLU()\n",
    "\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(64*10*10, 128)\n",
    "#         self.relu6=nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(128, 30)\n",
    "#         # self.relu4=nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.conv1(x))\n",
    "#         x = self.pool2(self.relu2(self.conv2(x)))\n",
    "#         x = self.relu3(self.conv3(x))\n",
    "#         x = self.pool4(self.relu4(self.conv4(x)))\n",
    "#         x = self.relu5(self.conv5(x))\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.fc2(self.relu6(self.fc1(x)))\n",
    "#         print('five')\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnmodel01 = ConvNet02()\n",
    "# cnnmodel01.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding metrics for each epoch\n",
    "# def metrics(loader, model):\n",
    "#     n_correct = 0\n",
    "#     n_total_samples = 0\n",
    "#     loss = 0\n",
    "#     n_minibatches = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images,labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = cnnmodel01(images)\n",
    "            \n",
    "#             loss += criterion(outputs, labels)\n",
    "#             n_minibatches += 1\n",
    "            \n",
    "#             #accuracy calculation\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             n_total_samples = labels.size(0)\n",
    "#             n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#     return loss/n_minibatches, n_correct/n_total_samples\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Single-batch training [8 marks]\n",
    "\n",
    "Write the foundational code which trains your network given **one single batch** of training data and computes the loss on the complete validation set for each epoch. Set ```batch_size = 64```. \n",
    "\n",
    "Display the graph of the training and validation loss over training epochs, showing as long as necessary to show you can drive the training loss to zero.\n",
    "\n",
    "> Please leave all graphs and code you would like to be marked clearly displayed without needing to run code cells or wait for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the loss function and suitable optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net01.parameters(),\n",
    "                            lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 1 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 2 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 3 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 4 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 5 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 6 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 7 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 8 training loss:  3.403 training accuracy:  3.1%  test loss:  3.389 test accuracy:  0.0%\n",
      "epoch: 9 training loss:  3.403 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 10 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 11 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 12 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 13 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 14 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 15 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 16 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 17 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 18 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 19 training loss:  3.402 training accuracy:  3.1%  test loss:  3.388 test accuracy:  0.0%\n",
      "epoch: 20 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 21 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 22 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 23 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 24 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 25 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 26 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 27 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 28 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 29 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 30 training loss:  3.402 training accuracy:  3.1%  test loss:  3.387 test accuracy:  0.0%\n",
      "epoch: 31 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 32 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 33 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 34 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 35 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 36 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 37 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n",
      "epoch: 38 training loss:  3.402 training accuracy:  3.1%  test loss:  3.386 test accuracy:  0.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/20/fhjypz_x5yb3__tx_g6cgjk00000gn/T/ipykernel_31942/4059728567.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(loader, net)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# accumulate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/20/fhjypz_x5yb3__tx_g6cgjk00000gn/T/ipykernel_31942/2261575034.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nepochs = 100\n",
    "statsrec = np.zeros((4,nepochs))\n",
    "results_path1 = root+'/results/cnnmodel01.pt'\n",
    "# single-batch\n",
    "it = iter(train_loader)\n",
    "input_s, label_s = next(it)\n",
    "inputs, labels = input_s.to(device), label_s.to(device)\n",
    "\n",
    "# training\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "    correct = 0          # number of examples predicted correctly (for accuracy)\n",
    "    total = 0            # number of examples\n",
    "    running_loss = 0.0   # accumulated loss (for mean loss)\n",
    "    n = 0                # number of minibatches\n",
    "    \n",
    "    \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward, backward, and update parameters\n",
    "    outputs = net01(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # accumulate loss\n",
    "    running_loss += loss.item()\n",
    "    n += 1\n",
    "    \n",
    "    # accumulate data for accuracy\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)    # add in the number of labels in this minibatch\n",
    "    correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
    "    \n",
    "    # collect together statistics for this epoch\n",
    "    ltrn = running_loss/n\n",
    "    atrn = correct/total \n",
    "    ltst, atst = stats(test_loader, net01)\n",
    "\n",
    "    statsrec[:,epoch] = (ltrn, atrn, ltst.cpu(), atst)\n",
    "    print(f\"epoch: {epoch} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\n",
    "\n",
    "# save network parameters, losses and accuracy\n",
    "torch.save({\"state_dict\": net01.state_dict(), \"stats\": statsrec}, results_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = torch.load(results_path1)\n",
    "statsrec = data[\"stats\"]\n",
    "plt.figure(figsize=(15,15));\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(statsrec[0], 'r', label = 'training loss', marker='*')\n",
    "plt.plot(statsrec[2], 'g', label = 'test loss' , marker='*')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and test loss, and test accuracy')\n",
    "ax2=ax1.twinx()\n",
    "ax2.plot(statsrec[1], 'm', label = 'training accuracy', marker='o')\n",
    "ax2.plot(statsrec[3], 'b', label = 'test accuracy', marker='o')\n",
    "ax2.set_ylabel('accuracy')\n",
    "plt.legend(loc='upper center')\n",
    "fig.savefig(root+'/001.svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epoch = 2\n",
    "# n_total_steps = len(train_loader)\n",
    "# resultpath = root+'/results/cnnmodel01.pt'\n",
    "\n",
    "# print('first')\n",
    "# iteration = iter(train_loader)\n",
    "# images_s, labels_s = next(iteration)\n",
    "# images, labels = images_s.to(device), labels_s.to(device)\n",
    "\n",
    "# print('second')\n",
    "# for epoch in range (num_epoch):\n",
    "#     n_correct = 0\n",
    "#     n_total_samples = 0\n",
    "#     r_loss = 0\n",
    "#     n_minibatches = 0\n",
    "    \n",
    "#     print('third')\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "    \n",
    "#     #forward pass\n",
    "#     outputs = cnnmodel01(images)\n",
    "#     loss = criterion(outputs, labels)\n",
    "\n",
    "#     #Backward and optimize\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     r_loss += loss.item()\n",
    "#     n_minibatches += 1\n",
    "    \n",
    "#      #accuracy calculation\n",
    "#     _, predicted = torch.max(images.data, 1)\n",
    "#     n_total_samples = labels.size(0)\n",
    "#     n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#     ltrn = r_loss/n_minibatches\n",
    "#     atrn = n_correct/n_total_samples \n",
    "#     ltst, atst = metrics(test_loader, cnnmodel01)\n",
    "\n",
    "#     #statsrec[:,epoch] = (ltrn, atrn, ltst.cpu(), atst)\n",
    "#     #print(f\"epoch: [{epoch+1}/{num_epoch}], training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\n",
    "\n",
    "\n",
    "#     print (f'Epoch [{epoch+1}/{num_epoch}] training loss: {ltrn: .3f},training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%} Step [{i+1}/{n_total_steps}] Loss: {loss.item():.4f}')\n",
    "\n",
    "# print('Finished Training')\n",
    "# # save network parameters, losses and accuracy\n",
    "# torch.save({\"state_dict\": cnnmodel01.state_dict()}, resultpath)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.2 Training on complete dataset [23 marks]\n",
    "\n",
    "### 1.2.1 Train CNN and show loss graph [6 marks]\n",
    "\n",
    "Train your model on the complete training dataset, and use the validation set to determine when to stop training.\n",
    "\n",
    "Display the graph of training and validation loss over epochs to show how you determined the optimal number of training epochs.\n",
    "\n",
    "> As in previous sections, please leave the graph clearly displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.2 Finetuning [6 marks]\n",
    "\n",
    "Now finetune your architecture by implementing at least 2 methods of reducing overfitting and increasing the model's ability to generalise. You are encouraged to further adjust the model after you have done the minimum requirement, to increase your model performance. Please do not use any pre-trained weights from a model trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1:** Data augmentation of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2:** Adding dropout and/or batch normalisation to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you adjust the Model class, redefine it below and instantiate it as ```model_122a```, ```model_122b```, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your new Model class\n",
    "# model_122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.3 Training comparison [4 marks]\n",
    "\n",
    "Display, side-by-side or on one single graph, the training and validation loss graphs for the single-batch training (section 1.1.3), on the full training set (1.2.1) and your final fine-tuned model (1.2.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what can be seen in the graphs.\n",
    "\n",
    "--> Double click here to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.4 Confusion matrices [7 marks]\n",
    "\n",
    "Use your architecture with best accuracy to generate two confusion matrices, one for the training set and one for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions can be drawn from the confusion matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Double click to respond here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.3 Testing on test data [18 marks]\n",
    "\n",
    "### 1.3.1 Dataset and generating predictions [6 marks]\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3.2 CSV file and test set accuracy [12 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image on test set and 1 row for the headers.\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [10 marks]. The class leaderboard will not affect marking (brownie points!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sR8Gc04CJ2"
   },
   "source": [
    "\n",
    "\n",
    "## QUESTION 2 [40 marks]\n",
    "\n",
    "\n",
    "\n",
    "In this question, you will visualize the filters and feature maps of a fully-trained CNN (AlexNet) on the full ImageNet 2012 dataset.\n",
    "\n",
    "> Please do not alter the name of the function or the number and type of its arguments and return values, otherwise the automatic grading function will not work correctly. You are welcome to import other modules (though the simplest solution only requires the ones below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-HIUgQ-HK8Y"
   },
   "source": [
    "### **Overview:**\n",
    "*   **2.1.1** Extract filters from model: ``fetch_filters(layer_idx, model)``\n",
    "*   **2.2.1** Load test image\n",
    "*   **2.2.2** Extract feature maps for given test image: ``fetch_feature_maps(image, model)``\n",
    "*   **2.2.3** Display feature maps\n",
    "*   **2.3.1** Generate Grad-CAM heatmaps: ``generate_heatmap(output, class_id, model, image)``\n",
    "*   **2.3.2** Display heatmaps: add code to cell\n",
    "*   **2.3.3** Generate heatmaps for failure analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gYdjXng4CJ5"
   },
   "source": [
    "### Loading a pre-trained model\n",
    "\n",
    "Run the cell below to load an AlexNet model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik9dzD4S4CJ6"
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXzlLbstCE7f"
   },
   "source": [
    "\n",
    "## 2.1 Extract and visualize the filters [6 marks]\n",
    "\n",
    "In this section you will extract and visualize the filters from the pre-trained AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vOrqr2J4CJ7"
   },
   "source": [
    "### 2.1.1 Extract filters [4 marks]\n",
    "\n",
    "Complete the following function ```fetch_filters``` to return all the filters from the convolutional layers at the given index in ```model.features``` (see printed model above for reference). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> We will not test the behaviour of your function using invalid indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdbDXckn4CJ8"
   },
   "outputs": [],
   "source": [
    "def fetch_filters(layer_idx, model):\n",
    "    \"\"\" \n",
    "        Args:\n",
    "            layer_idx (int): the index of model.features specifying which conv layer\n",
    "            model (AlexNet): PyTorch AlexNet object\n",
    "        Return:\n",
    "            filters (Tensor):      \n",
    "    \"\"\"\n",
    "    # TO COMPLETE\n",
    "    # return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNxPI5y-4CJ8"
   },
   "outputs": [],
   "source": [
    "# all the indices of the conv layers\n",
    "conv_layer_idx = [0, 3, 6, 8, 10]\n",
    "\n",
    "filters = []\n",
    "\n",
    "for layer_idx in conv_layer_idx:\n",
    "    filters.append(fetch_filters(layer_idx, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWSpX94c4CKD"
   },
   "source": [
    "For your testing purposes, the following code blocks test the dimensions of the function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXeQtKkK4CKE"
   },
   "outputs": [],
   "source": [
    "filters[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZnC8Eth4CKF"
   },
   "outputs": [],
   "source": [
    "assert list(filters[0].shape) == [64, 3, 11, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECx7Ktsg4CKG"
   },
   "source": [
    "\n",
    "\n",
    "### 2.1.2 Display filters [2 marks]\n",
    "\n",
    "The following code will visualize some of the filters from each layer. Play around with viewing filters at different depths into the network. Note that ```filters[0]``` could be viewed in colour if you prefer, whereas the subsequent layers must be viewed one channel at a time in grayscale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K6N3ThU4CKG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# limit how many filters to show\n",
    "to_show = 16\n",
    "\n",
    "# compute the dimensions of the plot\n",
    "plt_dim = int(math.sqrt(to_show))\n",
    "\n",
    "# plot the first channel of each filter in a grid\n",
    "for i, filt in enumerate(filters[0].numpy()[:to_show]):\n",
    "    plt.subplot(plt_dim, plt_dim, i+1)\n",
    "    plt.imshow(filt[0], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCJF9IIF4CKI"
   },
   "source": [
    "\n",
    "\n",
    "## 2.2 Extract and visualize feature maps [10 marks]\n",
    "\n",
    "In this section, you will pass a test image through the AlexNet and extract and visualize the resulting convolutional layer feature maps.\n",
    "\n",
    "Complete the following code cell to load the test image ```man_bike.JPEG```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVfEgbC4I_dE"
   },
   "source": [
    "### 2.2.1 Load test image [1 mark]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xypfUN7y4CKI"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF2t9uOk4CKJ"
   },
   "source": [
    "Run the code cell below to apply the image transformation expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt0tJQsM4CKO"
   },
   "outputs": [],
   "source": [
    "# ImageNet normalisation values, to apply to the image transform\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std),\n",
    "    ])\n",
    "\n",
    "im = data_transform(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQKNo384CKP"
   },
   "source": [
    "\n",
    "### 2.2.2 Extract feature maps [5 marks]\n",
    "\n",
    "Complete the function below to pass the test image through a single forward pass of the network. We are interested in the outputs of the max pool layers (outputs of conv layers at model.features indices 0, 3, and 10) for best visualization. Note that the input should pass through *every layer* of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmqQ_mJ54CKP"
   },
   "outputs": [],
   "source": [
    "def fetch_feature_maps(image, model):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image (Tensor): a single input image with transform applied\n",
    "        model (AlexNet): PyTorch AlexNet object\n",
    "        \n",
    "    Return:\n",
    "        feature_maps (Tensor): all the feature maps from conv layers \n",
    "                    at indices 0, 3, and 10 (outputs of the MaxPool layers)\n",
    "    \"\"\"\n",
    "\n",
    "    # TO COMPLETE\n",
    "    # return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORkRxCVo4CKQ"
   },
   "outputs": [],
   "source": [
    "feature_maps = fetch_feature_maps(im.unsqueeze(0), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf3SZoFu4CKQ"
   },
   "source": [
    "For your testing purposes, the following code block tests the dimensions of part of the function output. Note that the first dimension is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow7jGdQ94CKR"
   },
   "outputs": [],
   "source": [
    "assert len(feature_maps) == 3\n",
    "assert list(feature_maps[0].shape) == [1, 64, 31, 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDmwrp-w4CKR"
   },
   "source": [
    "\n",
    "\n",
    "### 2.2.3 Display feature maps [4 marks]\n",
    "\n",
    "Using the code for displaying filters as reference, write code in the block below to display the outputs of the first **16 feature maps from each of the 3 max-pool layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2O8TZG74CKS"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZNGf5WQ4CKG"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2.3 Understanding of filters and feature maps [7 marks]\n",
    "\n",
    "Respond in detail to the questions below. (Note that all text boxes can be formatted using Markdown if desired).\n",
    "\n",
    "### 2.3.1 [3 marks]\n",
    "Describe what the three filters at indices 0, 4, and 6 from the first convolutional layer are detecting (reference the corresponding feature maps to support your discussion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Double click here to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 [2 marks]\n",
    "Discuss how the filters change with depth into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Double click here to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 [2 marks]\n",
    "Discuss how the feature maps change with depth into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Double click here to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXeO_agI4CKS"
   },
   "source": [
    "\n",
    "## 2.4 Gradient-weighted Class Activation Mapping (Grad-CAM) [17 marks]\n",
    "\n",
    "In this section, we will explore using Gradient-weighted Class Activation Mapping (Grad-CAM) to generate coarse localization maps highlighting the important regions in the test images guiding the model's prediction. We will continue using the pre-trained AlexNet.\n",
    "\n",
    "#### Preparation\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "#### The AlexNet class\n",
    "\n",
    ">To implement Grad-CAM, we need to edit the AlexNet ```module``` class itself, so instead of loading the AlexNet model from ```torch.hub``` as we did above, we will use the official PyTorch AlexNet class code ([taken from here](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html)). In addition to the class definition, there is also a function below called ```alexnet()``` which allows you to specify whether you want the pretrained version or not, and if so, loads the weights. \n",
    "\n",
    "#### The hook\n",
    "\n",
    ">[Hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) in PyTorch are functions which can be registered, or attached, to a ```Module``` or ```Tensor```. Hooks can be *forward* hooks or *backward* hooks; forward hooks are called with ```forward()``` and backward hooks with ```backward()```. In the model below, we register a forward hook that saves the **gradients of the activations** to the Tensor output of ```model.features```. The gradients are saved to a class variable so we can easily access them.\n",
    "\n",
    "Carefully read the code block below. You do not need to add anything to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74G4wPeG4CKS"
   },
   "outputs": [],
   "source": [
    "# defining where to load the pre-trained weights from\n",
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-7be5be79.pth',\n",
    "}\n",
    "\n",
    "# the class definition\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # a placeholder for storing the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    # the hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        # stores the gradients of the hook's tensor to our placeholder variable\n",
    "        self.gradients = grad\n",
    "\n",
    "    # a method for extracting the activations of the last conv layer only (when we're \n",
    "    # not interested in a full forward pass)\n",
    "    def get_activations(self, x):\n",
    "        return self.features(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # we register the hook here to save the gradients of the last convolutional\n",
    "        # layer outputs\n",
    "        hook = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(pretrained=False, progress=True, **kwargs) -> AlexNet:\n",
    "    \"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FZEiLFv4CKT"
   },
   "outputs": [],
   "source": [
    "model = alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiBjgrST4CKT"
   },
   "outputs": [],
   "source": [
    "# pass our test image through our new model with the hook\n",
    "output = model(im.unsqueeze(0))\n",
    "\n",
    "# save the predicted class\n",
    "_, pred_cls = output.max(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzCYhGyI4CKT"
   },
   "source": [
    "Examine and understand the values stored in ```output``` and ```pred_cls```. What does AlexNet classify the test image as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU5QjsuP4CKU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr4svXcg4CKU"
   },
   "source": [
    "### 2.4.1 Generate Grad-CAM heatmaps [8 marks]\n",
    "\n",
    "With the hooks in place, now implement the code to generate Grad-CAM heatmaps, by following the guiding comments in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdlr69tw4CKU"
   },
   "outputs": [],
   "source": [
    "def generate_heatmap(output, class_id, model, image):\n",
    "    \n",
    "    # 1. compute the gradient of the score for the predicted class (logit)\n",
    "    # with respect to the feature map activations of the last convolutional layer\n",
    "    # Hint: calling .backward() on a Tensor computes its gradients\n",
    "    # TO COMPLETE\n",
    "    \n",
    "    # 2. get the gradients from the model placeholder variable\n",
    "    # TO COMPLETE\n",
    "    assert list(gradients.shape) == [1, 256, 7, 7]\n",
    "    \n",
    "    # pool the gradients across the channels\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    assert list(pooled_gradients.shape) == [256]\n",
    "    \n",
    "    # 3. get the activations of the last convolutional layer\n",
    "    # TO COMPLETE\n",
    "    assert list(activations.shape) == [1, 256, 7, 7]\n",
    "    \n",
    "    # 4. weight (multiply) the channels (dim=1 of activations) by the corresponding\n",
    "    # gradients (pooled_gradients)\n",
    "    # TO COMPLETE\n",
    "\n",
    "    # average the channels of the activations and squeeze out the extra dimension\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    assert list(heatmap.shape) == [7, 7]\n",
    "    \n",
    "    # 5. apply a ReLU to the linear combination of maps because we are only \n",
    "    # interested in the features that have a positive influence on the class of \n",
    "    # interest, i.e. pixels whose intensity should be increased in order to increase y\n",
    "    # Hint: you can use np.maximum() and torch.max() to perform ReLU if you prefer.\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    # return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PEmi-aM4CKU"
   },
   "outputs": [],
   "source": [
    "heatmap = generate_heatmap(output, pred_cls, model, im.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaiH3MIO4CKV"
   },
   "source": [
    "Check the dimensions of ```heatmap```. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qigb0A9F4CKV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvyCYUpw4CKW"
   },
   "source": [
    "### 2.4.2 Display heatmaps [4 marks]\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap super-imposed onto the original image. To get results as shown in the paper, we recommend the following steps:\n",
    "\n",
    "1. Resize the heatmap to match the size of the image.\n",
    "2. Rescale the image to a 0-255 integer range.\n",
    "3. Apply a colormap to the heatmap using ```cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)```.\n",
    "4. Multiply all values of heatmap by 0.4 to reduce colour saturation.\n",
    "5. Superimpose the heatmap onto the original image (Note: please perform cv2's addition - addition of two cv2 images, not numpy addition. See [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.html#:~:text=addWeighted()%20etc.-,Image%20Addition,OpenCV%20addition%20and%20Numpy%20addition.) for explanation.)\n",
    "6. Normalize the image between 0-255 again.\n",
    "7. Display the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwTIJoh84CKX"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf9aa-2V4CKX"
   },
   "source": [
    "Show the heatmap for class ```'seashore, coast, seacoast, sea-coast'``` (```class_id = 978```), super-imposed onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Failure analysis using Grad-CAM [5 marks]\n",
    "\n",
    "Find an image (online, or from ImageNet or another dataset) which AlexNet classifies *incorrectly*. Display the image below, and show the model's predicted class. Then, generate the Grad-CAM heatmap and display it super-imposed onto the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe what explanation the Grad-CAM heatmap provides about why the model has failed to correctly classify your test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Double click to respond here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Overall quality [2 marks]\n",
    "\n",
    "Marks awarded for overall degree of code readibility and omission of unnecessary messy outupts (for example, please avoid printed losses for every batch of a long training process, large numpy arrays, etc.) throughout the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd03d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
